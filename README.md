# Tokenizer
Building a Tokenizer for Large Language Models

tokenizer.ipynb: Building BERT, GPT-2, and XLNet tokenizers, with three main tokenization algorithms: WordPiece (Bert), BPE (GPT-2), and Unigram (XLNet). 

Tokenizer.json: Model saved.

Wikitext-2.txt: dataset.

Based don: https://huggingface.co/learn
