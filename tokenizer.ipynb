{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# <h3 align=\"center\"> $\\underline{\\text{ Tokenizer}}$</h3>\n",
        "\n",
        "<h3 align=\"center\">$ \\text{Tokenization for Large Language Models}$</h3>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qotdn1FfJvch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "xJTGhS0sfski"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "3sCg0ZJ-gIqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building BERT, GPT-2, and XLNet tokenizers, with three main tokenization algorithms: WordPiece (Bert), BPE (GPT-2), and Unigram (XLNet)."
      ],
      "metadata": {
        "id": "p9K_fWv1Beea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a BERT tokenizer (WorldPiece)"
      ],
      "metadata": {
        "id": "jwsxaKm2Pdyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Gather a Corpus\n",
        "Dataset: Wikitext"
      ],
      "metadata": {
        "id": "XMVaBiINOhA4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXow7RyXef-u"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the tokenizer, we need a generator that will yield batches of 1,000 texts."
      ],
      "metadata": {
        "id": "Dv8RSBG0dE38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTCorpus():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]"
      ],
      "metadata": {
        "id": "wz6ooY_kQpap"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generateinga text file that contains all the texts-inputs from the dataset to use locally"
      ],
      "metadata": {
        "id": "bl16qCrzg3gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as t:\n",
        "    for i in range(len(dataset)):\n",
        "        t.write(dataset[i][\"text\"] + \"\\n\")"
      ],
      "metadata": {
        "id": "uKqgXtBsQuhC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a backend_tokenizer with *tokenizers*"
      ],
      "metadata": {
        "id": "nPSLknFTiAVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import (decoders,models,normalizers,pre_tokenizers,processors,trainers,Tokenizer,)\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
      ],
      "metadata": {
        "id": "-ATn_ALPgVv3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPrX_ovyQhxC"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a) Normalization: By hand you can compose several normalizers using *Sequence()*"
      ],
      "metadata": {
        "id": "SdcT6IcwnFad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)  #prebuilt\n",
        "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])"
      ],
      "metadata": {
        "id": "bMq5iVYJgVqz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out the normalizer\n",
        "print(tokenizer.normalizer.normalize_str(\"HÖw Äre YÖü?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9iZWjQNmxOY",
        "outputId": "9f9bd154-0812-416f-96df-a8248e4c829a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Pre-tokenization"
      ],
      "metadata": {
        "id": "7pXACaTppTue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()  #prebuilt\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
      ],
      "metadata": {
        "id": "pP8vSs2BmxK2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting on whitespace and punctuation:\n",
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"I'm fine, thanks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ3TS7ZWpsw-",
        "outputId": "82e4bb04-5641-4e28-85d6-3b2cacc9e350"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', (0, 1)),\n",
              " (\"'\", (1, 2)),\n",
              " ('m', (2, 3)),\n",
              " ('fine', (4, 8)),\n",
              " (',', (8, 9)),\n",
              " ('thanks', (10, 16)),\n",
              " ('!', (16, 17))]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting only on whitespace:\n",
        "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "pre_tokenizer.pre_tokenize_str(\"I'm fine, thanks!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsBmc1a4r-BT",
        "outputId": "54d8583e-209c-4db0-d643-0eeba5c8a951"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"I'm\", (0, 3)), ('fine,', (4, 9)), ('thanks!', (10, 17))]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequence to compose several pre-tokenizers:\n",
        "pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()])\n",
        "pre_tokenizer.pre_tokenize_str(\"I'm fine, thanks-so-much!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIDqODw2sgJa",
        "outputId": "7eff9fd4-d9b0-4461-8949-124130b5533c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', (0, 1)),\n",
              " (\"'\", (1, 2)),\n",
              " ('m', (2, 3)),\n",
              " ('fine', (4, 8)),\n",
              " (',', (8, 9)),\n",
              " ('thanks', (10, 16)),\n",
              " ('-', (16, 17)),\n",
              " ('so', (17, 19)),\n",
              " ('-', (19, 20)),\n",
              " ('much', (20, 24)),\n",
              " ('!', (24, 25))]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Model"
      ],
      "metadata": {
        "id": "HEbuqeB_pXr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We require the *WordPieceTrainer*. You need to pass all the special tokens you intend to use"
      ],
      "metadata": {
        "id": "PDw1BRPq6bQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running the inputs through the model\n",
        "special_tokens = [\"[SEP]\", \"[MASK]\", \"[UNK]\", \"[PAD]\", \"[CLS]\"]\n",
        "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
      ],
      "metadata": {
        "id": "X0cCD6v2mxHQ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train_from_iterator(getTCorpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "OZMHItvh7LnN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the Tokenizer\n",
        "encod = tokenizer.encode(\"Testing the tokenizer!\")\n",
        "print(encod.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5klSDok7cEF",
        "outputId": "965e3209-5843-40fc-8944-4f6e1a1223a8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['testing', 'the', 'tok', '##eni', '##zer', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to know the IDs of the [CLS] and [SEP] tokens\n",
        "cls_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(cls_id, sep_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYF3PP9F8bgf",
        "outputId": "2d3f0016-1e46-4fdf-b214-236a4ef9f1dd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TemplateProcessor: specify how to treat a single sentence and a pair of sentences"
      ],
      "metadata": {
        "id": "PCVMSWGb9Xb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Special tokens\n",
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single=f\"[CLS]:0 $A:0 [SEP]:0\",             # The first single sentence is represented by $A\n",
        "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",  # The second sentence (if pair) is represented by $B\n",
        "    special_tokens=[(\"[CLS]\", cls_id), (\"[SEP]\", sep_id)],)"
      ],
      "metadata": {
        "id": "nKNiDSMj9rWO"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encod = tokenizer.encode(\"Testing the tokenizer!\", \"Yes, why not?\")\n",
        "print(encod.tokens)\n",
        "print(encod.type_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnMqhX2f-m1H",
        "outputId": "0f052864-1469-42ca-e9d7-05b2d26bd9b0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', 'testing', 'the', 'tok', '##eni', '##zer', '!', '[SEP]', 'yes', ',', 'why', 'not', '?', '[SEP]']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Postprocessor\n",
        "\n",
        "Including a decoder:"
      ],
      "metadata": {
        "id": "EWkDs6VrpbXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
      ],
      "metadata": {
        "id": "aFGOqUFdmxES"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encod.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x9shtMQSmxA8",
        "outputId": "cf4e673e-1f17-49f3-c926-1e2587ed2eb3"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'testing the tokenizer! yes, why not?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving our tokenizer in a single JSON:"
      ],
      "metadata": {
        "id": "PN1Ry3aUAUXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "pS6LDlJOmw9d"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load the backend_tokenizer in a *transformers tokenizer*"
      ],
      "metadata": {
        "id": "oYJ2DOdOcy1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
      ],
      "metadata": {
        "id": "OcABY2N8mw53"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then use this tokenizer like any other tokenizer."
      ],
      "metadata": {
        "id": "U-X_xs1gEisD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "#set all the special tokens\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,tokenizer_file=\"tokenizer.json\",\n",
        "    unk_token=\"[UNK]\",pad_token=\"[PAD]\",cls_token=\"[CLS]\",sep_token=\"[SEP]\",mask_token=\"[MASK]\",)"
      ],
      "metadata": {
        "id": "uSgzpFQuA7V8"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building GPT-2 Tokenizer (BPE)"
      ],
      "metadata": {
        "id": "xw2Cp00AFDYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a backend_tokenizer with *tokenizers*"
      ],
      "metadata": {
        "id": "kVw4sQ9jHjHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT-2 uses byte-level BPE\n",
        "\n",
        "GPT-2 does not use a normalizer\n",
        "\n",
        "GPT-2 the only special token is the end-of-text token"
      ],
      "metadata": {
        "id": "zNUvRIrmIm2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.BPE())"
      ],
      "metadata": {
        "id": "B2i9YvXWEBW6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-tokenization\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
      ],
      "metadata": {
        "id": "toz0Mvf1EBDr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Testing pre-tokenization!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyPdd6xlJMhB",
        "outputId": "d222f20f-1bf2-435d-e43d-8375cc128fe0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Testing', (0, 7)),\n",
              " ('Ġpre', (7, 11)),\n",
              " ('-', (11, 12)),\n",
              " ('tokenization', (12, 24)),\n",
              " ('!', (24, 25))]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The model to be training\n",
        "trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.train_from_iterator(getTCorpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "av2CkF-qJMSO"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "tokenizer.model = models.BPE()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "E676JIc3J8fX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encod = tokenizer.encode(\"Testing the super-tokenizer.\")\n",
        "print(encod.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do1k_WN_J8H5",
        "outputId": "5c53990b-2b1d-471f-d90f-15ca4d2a5c57"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['T', 'est', 'ing', 'Ġthe', 'Ġsuper', '-', 't', 'oken', 'izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Byte-level post-processing for the GPT-2 tokenizer\n",
        "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
      ],
      "metadata": {
        "id": "OhWf_wqEKqz2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Testing the super-tokenizer.\"\n",
        "encoding = tokenizer.encode(sentence)\n",
        "start, end = encoding.offsets[4]\n",
        "sentence[start:end]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xJ-5-kksKxff",
        "outputId": "93182067-0c99-45de-9f7a-c99e26c72596"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' super'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding byte-level decoder\n",
        "tokenizer.decoder = decoders.ByteLevel()"
      ],
      "metadata": {
        "id": "eYU9a2shKxa7"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoding.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s1fffZBxL1Ua",
        "outputId": "251fd35f-e7ef-4612-84a2-0e3d708cad18"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Testing the super-tokenizer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer and wrap it\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,bos_token=\"<|endoftext|>\",\n",
        "    eos_token=\"<|endoftext|>\",)"
      ],
      "metadata": {
        "id": "9XSRX3KZL8SI"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building XLNet (Unigram)"
      ],
      "metadata": {
        "id": "Zz1-hvieGnHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create a backend_tokenizer with *tokenizers*"
      ],
      "metadata": {
        "id": "JeD7VOJXIFej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(models.Unigram())"
      ],
      "metadata": {
        "id": "8nYa1E6pIFHY"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Regex\n",
        "\n",
        "tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(\"``\", '\"'),normalizers.Replace(\"''\", '\"'),\n",
        "        normalizers.NFKD(),normalizers.StripAccents(),normalizers.Replace(Regex(\" {2,}\"), \" \"),])"
      ],
      "metadata": {
        "id": "4wzxcBr-MnFK"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
      ],
      "metadata": {
        "id": "jo0QXxqLMvSx"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"Testing the pre-tokenizer!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCfvDNM8MvN2",
        "outputId": "f8e9cffb-ea55-409b-c0cb-1dd1440d1bc3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('▁Testing', (0, 7)), ('▁the', (7, 11)), ('▁pre-tokenizer!', (11, 26))]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\n",
        "trainer = trainers.UnigramTrainer(vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\")\n",
        "tokenizer.train_from_iterator(getTCorpus(), trainer=trainer)"
      ],
      "metadata": {
        "id": "BGQHmjNvIEk7"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model = models.Unigram()\n",
        "tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
      ],
      "metadata": {
        "id": "LyoyG2EiNTdd"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Testing this tokenizer.\")\n",
        "print(encoding.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rk5SYZ44NTX7",
        "outputId": "41f8d0c1-04a7-4deb-d2ca-2b04917e5821"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Test', 'ing', '▁this', '▁to', 'ken', 'izer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_id = tokenizer.token_to_id(\"<cls>\")\n",
        "sep_id = tokenizer.token_to_id(\"<sep>\")\n",
        "tokenizer.post_processor = processors.TemplateProcessing(single=\"$A:0 <sep>:0 <cls>:2\",pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
        "    special_tokens=[(\"<sep>\", sep_id), (\"<cls>\", cls_id)],)"
      ],
      "metadata": {
        "id": "sKnjwuVxNTSK"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode(\"Test the tokenizer.!\", \"on a pair of sentences...\")\n",
        "print(encoding.tokens)\n",
        "print(encoding.type_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NSbBtPhNtXO",
        "outputId": "efd93067-6228-4451-e975-ed07967a5d4a"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Test', '▁the', '▁to', 'ken', 'izer', '.', '!', '<sep>', '▁', 'on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '.', '.', '.', '<sep>', '<cls>']\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decoder = decoders.Metaspace()"
      ],
      "metadata": {
        "id": "zVZ8me_lNtUI"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,bos_token=\"<s>\",eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",pad_token=\"<pad>\",cls_token=\"<cls>\",sep_token=\"<sep>\",mask_token=\"<mask>\",padding_side=\"left\",)"
      ],
      "metadata": {
        "id": "bMKiu2PsNtPC"
      },
      "execution_count": 98,
      "outputs": []
    }
  ]
}